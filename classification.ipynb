{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"bgsedsc_0.jpg\">\n",
    "$\\newcommand{\\bb}{\\boldsymbol{\\beta}}$\n",
    "$\\DeclareMathOperator{\\Gau}{\\mathcal{N}}$\n",
    "$\\newcommand{\\bphi}{\\boldsymbol \\phi}$\n",
    "$\\newcommand{\\bpi}{\\boldsymbol \\pi}$\n",
    "$\\newcommand{\\bx}{\\boldsymbol{x}}$\n",
    "$\\newcommand{\\by}{\\boldsymbol{y}}$\n",
    "$\\newcommand{\\bmu}{\\boldsymbol{\\mu}}$\n",
    "$\\newcommand{\\bS}{\\boldsymbol{\\Sigma}}$\n",
    "$\\newcommand{\\whbb}{\\widehat{\\bb}}$\n",
    "$\\newcommand{\\hf}{\\hat{f}}$\n",
    "$\\newcommand{\\hy}{\\hat{y}}$\n",
    "$\\newcommand{\\tf}{\\tilde{f}}$\n",
    "$\\newcommand{\\ybar}{\\overline{y}}$\n",
    "$\\newcommand{\\E}{\\mathbb{E}}$\n",
    "$\\newcommand{\\Var}{Var}$\n",
    "$\\newcommand{\\Cov}{Cov}$\n",
    "$\\newcommand{\\Cor}{Cor}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "In some ways there is very little to say about classification: it is like regression but with categorical response. As before there will be a learning function $f(\\bx)$ but now the model that relates it to the response has to change to reflect the different characteristics thereof. But we can use the same set of tools - loss functions will change and as a result our tools of measuring performance too. \n",
    "\n",
    "On the other hand, this is a very common prediction problem and it is worth to understand deeper some of its intricacies\n",
    "\n",
    "We will start with binary classification - the response is one of two categories. The labelling of the categories is arbitrary and any sensible methodology should not rely on how these categories are coded numerically. We will stick to 0/1 coding for the two categories. This is mathematically more convenient for the methods we will use here. For other approaches to classification -1/1 might be more convenient. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this module we build predictive models for categorical outputs, following the same paradigm as in regression but changing the distribution that relates the output $y$ to the learning function $f(\\bx)$. We also adapt appropriately the model performance criteria and introduce concepts such as the misclassification probability, the ROC curve and AUC score. We discuss the problem of class imbalance and some solutions. We also contrast regression with Bayes classifiers. We show how to predict multicategorical and ordinal output in a simple framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The spam dataset\n",
    "\n",
    "This is a classic dataset for binary classification, it can be found in the UCI repository\n",
    "\n",
    "http://www.ics.uci.edu/~mlearn/MLRepository.html\n",
    "\n",
    "and it is analyzed in few different ways in the Hastie et al. book \n",
    "\n",
    "I have created a version with a subset of the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "# This is a Python module that contains plotting commands\n",
    "import matplotlib.pyplot as plt\n",
    "# the following provides further tools for plotting with dfs\n",
    "import seaborn as sns \n",
    "\n",
    "import helper_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_george</th>\n",
       "      <th>word_freq_you</th>\n",
       "      <th>word_freq_your</th>\n",
       "      <th>word_freq_hp</th>\n",
       "      <th>word_freq_free</th>\n",
       "      <th>word_freq_hpl</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_re</th>\n",
       "      <th>word_freq_edu</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.778</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.47</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.372</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.276</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.137</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.135</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_george  word_freq_you  word_freq_your  word_freq_hp  \\\n",
       "0               0.0           1.93            0.96           0.0   \n",
       "1               0.0           3.47            1.59           0.0   \n",
       "2               0.0           1.36            0.51           0.0   \n",
       "3               0.0           3.18            0.31           0.0   \n",
       "4               0.0           3.18            0.31           0.0   \n",
       "\n",
       "   word_freq_free  word_freq_hpl  word_freq_our  word_freq_re  word_freq_edu  \\\n",
       "0            0.32            0.0           0.32          0.00           0.00   \n",
       "1            0.14            0.0           0.14          0.00           0.00   \n",
       "2            0.06            0.0           1.23          0.06           0.06   \n",
       "3            0.31            0.0           0.63          0.00           0.00   \n",
       "4            0.31            0.0           0.63          0.00           0.00   \n",
       "\n",
       "   word_freq_remove  char_freq_!  class  \n",
       "0              0.00        0.778      1  \n",
       "1              0.21        0.372      1  \n",
       "2              0.19        0.276      1  \n",
       "3              0.31        0.137      1  \n",
       "4              0.31        0.135      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam = pd.read_csv(\"spam_small_train.csv\")\n",
    "spam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f10b1de5898>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lets explore some basic aspects of this dataset\n",
    "\n",
    "spam[\"class\"].value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         AxesSubplot(0.1,0.15;0.363636x0.75)\n",
       "1    AxesSubplot(0.536364,0.15;0.363636x0.75)\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spam.groupby(\"class\").boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['word_freq_george', 'word_freq_you', 'word_freq_your', 'word_freq_hp',\n",
       "       'word_freq_free', 'word_freq_hpl', 'word_freq_our', 'word_freq_re',\n",
       "       'word_freq_edu', 'word_freq_remove', 'char_freq_!', 'class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recall that\n",
    "\n",
    "spam.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous exploratory analysis is analogous to screening by correlations in regression. We could for example test nonparametrically equality of the two distributions - although these are only assessing marginal dependence and not interactions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic classification using logistic regression\n",
    "\n",
    "We will follow a very analogous route to regression. There will be a learning function and we will take to be linear in parameters and features: \n",
    "\n",
    "$$ f(\\bx_i,\\bb) = \\bb^T \\bphi_i$$\n",
    "\n",
    "We will also model the distribution of the response and relate this distribution to the linear predictor\n",
    "\n",
    "There is no assumptions in this case for the distribution of the response: there is only one distribution to describe binary variables, the *Bernoulli*. Our probabilistic binary classification models take: \n",
    "\n",
    "$$ y_i \\sim Bernoulli(\\pi_i) \\iff p(y_i=1) = \\pi_i \\quad p(y_i=0) = 1-\\pi_i$$\n",
    "\n",
    "The remaining assumption we will make is how to relate $\\pi_i$ to the linear predictor. We basically need a transformation that maps real values to $[0,1]$. The *logistic transformation* is one option and leads to the so-called **logistic regression**:\n",
    "\n",
    "\\begin{equation}\n",
    "y_i \\sim Bernoulli\\left({1 \\over 1 + e^{-f(\\bx_i,\\bb)}}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "The function that maps $f(\\bx_i,\\bb)$ to $\\pi_i$ is shown below: \n",
    "\n",
    "<img src=\"480px-Logistic-curve.png\" width = 300>\n",
    "\n",
    "Basic math shows that in this model \n",
    "\n",
    "$$- \\log p(y_i | \\bx_i) =  \\log (1+e^{-f(\\bx_i,\\bb)}) + y_i f(\\bx_i,\\bb) $$ \n",
    "\n",
    "hence, the loss function becomes \n",
    "\n",
    "$$L(\\bb) = \\sum_i \\log (1+e^{-f(\\bx_i,\\bb)}) + y_i f(\\bx_i,\\bb) $$\n",
    "\n",
    "This is also **convex** and can be optimized efficiently (this true for other *link* functions too, e.g. probit). A standard way to do this is using Fisher scoring, a variation of Newton-Raphson - these are **gradient-descent iterative optimization algorithms**. These work in the same way for the whole family of **generalized linear models**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit the model to the spam dataset. For the moment we will use the original variables as features\n",
    "\n",
    "Here we focus on predictive modelling and the `LogisticRegression` is reasonable. For inference this is not providing enough detail. This function actually optimizes over a penalized likelihood loss function, the default being a rigde penalty. We will set the regularization parameter to a very small value to keep its effect minimal for the time being ($n$ here is very large relative to $p$)\n",
    "\n",
    "Note that unlike `LinearRegression` now we can feed the function with dfs and series... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "F = spam.drop('class', axis = 1)\n",
    "y = spam[\"class\"]\n",
    "\n",
    "# to print stats\n",
    "feature_names = F.columns\n",
    "class_labels = [\"email\",\"spam\"] # meant to represent 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     0.850964\n",
      "2     0.666129\n",
      "3     0.768705\n",
      "4     0.767645\n",
      "5     0.312271\n",
      "6     0.863007\n",
      "7     0.314139\n",
      "8     0.746933\n",
      "9     0.738770\n",
      "10    0.976138\n",
      "11    0.870806\n",
      "12    0.905789\n",
      "13    0.900109\n",
      "14    0.999671\n",
      "Name: pi_i, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# importing the relevant sklearn tools\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lregr = LogisticRegression(penalty='l2', C=100.0, \n",
    "                           fit_intercept=True, \n",
    "                           intercept_scaling=1, \n",
    "                           solver='liblinear', max_iter=500)\n",
    "\n",
    "# Fiting logistic regression\n",
    "\n",
    "lregr.fit(F,y)      \n",
    "\n",
    "# Compute the predicted probabilities in-sample\n",
    "\n",
    "insample_pred = lregr.predict_proba(F)\n",
    "\n",
    "insample_pred_res = spam.copy()\n",
    "\n",
    "insample_pred_res[\"pi_i\"] = insample_pred[:,1]\n",
    "\n",
    "print(insample_pred_res.iloc[1:15,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f10b1e98c18>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lets see more globally the fitted probs\n",
    "\n",
    "insample_pred_res.boxplot(column=\"pi_i\",by=\"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning probabilities into class prediction\n",
    "\n",
    "The *probabilistic classifier* we use explicitly accounts for misclassification errors. The algorithm returns probabilities and these also reflect the classification uncertainty\n",
    "\n",
    "Recall that if $y \\sim Bernoulli(\\pi)$ then $\\Var(y) = \\pi (1- \\pi)$, which is large for $\\pi \\approx 1/2$\n",
    "\n",
    "Often this is all one needs to have to do sensible data analysis - the probabilities (*see advanced material for details*)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In other contexts it might be of interest to turn probabilities into class predictions, $\\hy_i \\in \\{0,1\\}$, e.g., to report medical tests (pregnant/no pregnant) - the spam example is also a good example: a decision has to be taken for each email and it is convenient to have the algorithm produce class predictions. We will denote those by $\\hy_i$\n",
    "\n",
    "With class prediction there will be **misclassification errors**: false positives and false negatives\n",
    "\n",
    "Before we go into details, lets see what the default operations in `sklearn` do for us. We can compute the following quantities in or out of sample and cross-validated too - basically all the discussion about use of sample for evaluation for regression applies here too. For the moment we experiment with in-sample calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the predict method in the LogisticRegression object\n",
    "y_hat = lregr.predict(F)\n",
    "y_hat[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#confusion matrix\n",
    "cm =  confusion_matrix(y_pred=y_hat, y_true=y, labels=[0,1])\n",
    "print (cm)\n",
    "# Plotting confusion matrix (custom help function)\n",
    "\n",
    "plot_confusion_matrix(cm, class_labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What `.predict` has done is the following: if $\\pi_i \\geq 1/2$ it sets $\\hy_i = 1$. This is verified below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insample_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_new = np.ones(y.size)\n",
    "y_hat_new[insample_pred[:,1] <0.5] = 0\n",
    "#confusion matrix\n",
    "\n",
    "cm =  confusion_matrix(y_pred=y_hat_new, y_true=y, labels=[0,1])\n",
    "print (cm)\n",
    "# Plotting confusion matrix \n",
    "plot_confusion_matrix(cm, class_labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the application the losses can be very different (remember the medical example). Let \n",
    "\n",
    "$$L(true,predict)$$ \n",
    "\n",
    "be a loss function that computes the cost of misclassification - this is something that requires context information not in the data. We take \n",
    "\n",
    "$$L(0,0) = L(1,1) = 0 \\quad L(1,0) > 0 \\quad L(0,1) > 0$$\n",
    "\n",
    "Little math shows that if $L(1,0)/L(0,1) = C$ the optimal decision is: \n",
    "\n",
    "$$\\hy_i = 1 \\iff \\pi_i > {1 \\over 1+C}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misclassification rate\n",
    "\n",
    "$$p[ y \\neq \\hy]$$ \n",
    "\n",
    "is known as the misclassification probability. This can be estimated from data very much the same way as $R^2$: in sample (which can be negatively biased), out of sample (which is data intensive), by cross-validation etc; also by using more advanced math, such as *concentration inequalities*\n",
    "\n",
    "This number in isolation means pretty much nothing. Consider or example the (all too common situation) where we wish to predict $y$ in a population such that $p[y=1] = 0.001$. Then classifying everyone as $\\hy = 0$ yields a misclassification probability 0.001 but the algorithm is never able to identify the class of interest. Missclassification rate can be useful in comparisons. \n",
    "\n",
    "There are other performance metrics - related to conditional probabilities - e.g. $p[\\hy =1 | y=1]$ etc - such as specificity/sensitivity for qualifying the performance of a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve and AUC\n",
    "\n",
    "A common tool to assess the performance of a probabilistic classifier is the ROC curve. Each point on this curve is (an estimate of) pair\n",
    "\n",
    "$$\n",
    "(p([\\hy = 1 | y = 0] , p[\\hy =1 | y = 1]) \n",
    "$$\n",
    "\n",
    "For a given threshold $t$ we can estimate these probabilities from the confusion matrix - obtained in the best way we can - simply by computing the associated frequencies\n",
    "\n",
    "As we vary the threshold the confusion matrices change and the frequencies too: varying $t$ from 0 to 1 we obtain the ROC curve - read the figure from right to left\n",
    "\n",
    "We do this now using a customized function. I will do it on a test spam data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the test data, extract the info and create predictions\n",
    "\n",
    "spam_test = pd.read_csv(\"spam_small_test.csv\")\n",
    "Ftest = spam_test.drop(\"class\",axis=1)\n",
    "ytest = spam_test[\"class\"]\n",
    "\n",
    "test_pred = lregr.predict_proba(Ftest)\n",
    "\n",
    "# Custom plot function\n",
    "get_auc(ytest, test_pred, class_labels, column=1, plot=True) # Helper function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC is the Area Under (the ROC) Curve\n",
    "\n",
    "It has though an interesting and solid statistical interpretation. It can be directly related to a non-parametric test - the **Mann-Whitney** that the following two samples come from the same distribution: \n",
    "\n",
    "$$\n",
    "\\textrm{class 1 probs }: \\{\\pi_i: y_i = 1\\} \\quad \\textrm{class 0 probs }: \\{\\pi_i: y_i = 0\\}\n",
    "$$\n",
    "\n",
    "You would expect that for a decent classifier the two samples come from different distributions and the distribution of the \"class 1 probs\" is stochastically greater. If the two distributions were identical we would obtain the green-dashed ROC curve.\n",
    "\n",
    "Consider a contest between the two samples:  each element of the first we compare with all of the elements in the second and record how many times it was at least as big. AUC is the frequency of won contests!\n",
    "\n",
    "For some `sklearn` tools check \n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "As a sanity check, draw the curve for estimates using in-sample predictions.\n",
    "\n",
    "What do expect in-sample AUC to be, higher or lower than out-of-sample AUC? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Class imbalance\n",
    "\n",
    "### Context\n",
    "\n",
    "Typically, we want to predict the incidence of a rare event:  rare disease, an accident, a default, exceptional performance, etc, on the basis of measured characteristics. \n",
    "\n",
    "If we collect data *prospectively* we will end up with a sample with very little representation of the class we are particularly interested in. Hence it will be hard to learn the function that separates \"rare\" from \"common\" on the basis of the measured characteristics; indeed classifiers will not mind overpredicting the \"common\". \n",
    "\n",
    "Lets look at an example first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we predict good wine ? \n",
    "\n",
    "This is another standard dataset available from the UCI repository. It is really about **ordinal regression** - or maybe **multiclass classification**. Here we will for illustration consider (at the expense of losing information) a transformation of the response \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wine_df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\", sep=';')\n",
    "wine_df = pd.read_csv(\"wine.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df.quality.value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df['quality'] = [1 if q >= 8 else 0 for q in wine_df.quality ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df.quality.value_counts().plot(kind=\"bar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df.quality.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is class imbalance for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data\n",
    "X = wine_df.drop('quality', axis =1)\n",
    "y = wine_df.quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# I should have standardised features but here I am using tiny regularisation so it should not matter\n",
    "model = LogisticRegression(C=100, solver='liblinear') \n",
    "y_pred = cross_val_predict(model, X, y, cv = 5)\n",
    "model.fit(X,y)\n",
    "y_prob = model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC\n",
    "y_probabilities = cross_val_predict(model, X, y, method='predict_proba', cv = 5)\n",
    "get_auc(y, y_probabilities, [\"Bad/Average Wine\", \"Great Wine\"], column=1, plot=True) # Help function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "print (\"Accuracy (cross-validated): \", accuracy_score(y, y_pred))\n",
    "\n",
    "####  Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print (classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The operation was successful but the patient died!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "Plot the confustion matrix to verify results. Are we predicting the 'good wines'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some approaches to class imbalance\n",
    "\n",
    "Roughly speaking there are two ways to try and do better: \n",
    "\n",
    "+ Retrospective study & bias correction: this operates at the *experimental design* stage, that is the way data are collected in the first place \n",
    "\n",
    "+ Resampling and use of synthetic data (& bias correction): this works with the data at hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrospective studies\n",
    "\n",
    "Collect data not as a *representative* sample from the population of interest, but oversample the rare class; for example from your medical database choose a sample of $n/2$ patients with the rare disease and $n/2$ without\n",
    "\n",
    "On the basis of this *biased (non-representative)* sample train a probabilistic classifier, e.g., logistic regression \n",
    "\n",
    "The estimated learning function is biased too - it will predict way larger probability of class=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are at least two ways to do proper inference with the non-representative sample. \n",
    "\n",
    "Lets say that $q(y)$ are the probabilities of the two classes in the population of interest, i.e., $q(1), q(0)$. And $r(y)$ are the probabilities with which we have sampled \n",
    "\n",
    "+ One is to change the loss function: instead of using the log-likelihood, which is an *arithmetic average* of individual log-densities, use a *weighted average*\n",
    "\n",
    "  Some math shows that the following is a valid choice: \n",
    "  \n",
    "  $$ L(\\bb) = \\sum_i {q(y_i) \\over r(y_i)} \\left [\\log (1+e^{-f(\\bx_i,\\bb)}) + y_i f(\\bx_i,\\bb)\\right ] $$\n",
    "  \n",
    "+ Another is to run the analysis with the biased sample and the log-likelihood loss, but then rescale the estimated probabilities\n",
    "\n",
    "  Some math shows that if $\\pi(y_i)$ are the probabilities estimated by the model, they should be changed to \n",
    "  \n",
    "  $$ { \\pi(y_i) {q(y_i) \\over r(y_i)} \\over \\pi(1) {q(1) \\over r(1)} + \\pi(0) {q(0) \\over r(0)}}$$\n",
    "  \n",
    "  Lets understand what is the effect of this weighting: consider very small and very large predicted $\\pi_i$\n",
    "  \n",
    "Both approaches require that $q(y)$ is known - but this is often easy enough "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling approaches\n",
    "\n",
    "Having no control of the data collection protocol, one can try and sharpen the distinction between the two classes by either under-representing (*undersampling*) the popular class, or over-representing (*oversampling*) the rare class, or both\n",
    "\n",
    "Oversampling might just be randomly replicating rare cases or creating synthetic rare cases that \"look like\" the rare cases in the sample. To this respect doing some modelling on the $\\bx_i$s can help - and there are links to *Bayes classifiers* we mention later. For example SMOTE (Synthetic Minority Oversampling Technique) does this\n",
    "\n",
    "Any of these approaches needs to be combined with one of the bias-correction approaches developed above\n",
    "\n",
    "Some tools in `sklearn` to do this are, e.g., the module `imblearn` and its methods `.over_sampling`, e.g. `RandomOverSampler` and `SMOTE`. `imblearn` requires installing - do not do this now! Analogous result to `.over_sampling` is obtained using `LogisticRegression`  `class_weight=\"balanced\"` argument, that corresponds to oversampling\n",
    "\n",
    "In our wine prediction example we have also done some arbitrary dichotomization of the response: no good reason why we should not pay for it!!\n",
    "\n",
    "Lets try this one for the wine data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(C=100, class_weight='balanced', solver='liblinear')\n",
    "model.fit(X,y)\n",
    "y_prob_imb = model.predict_proba(X)\n",
    "wine_df_original = pd.read_csv(\"wine.csv\")\n",
    "wine_df_original[\"pred_prob\"] = y_prob[:,1] \n",
    "wine_df_original[\"pred_prob_imb\"] = y_prob_imb[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these are not proper estimates of the class probability: we need to correct for the biased sample by rescaling the predicted probability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the correction factor: \n",
    "\n",
    "q1 = y.sum()/len(y)\n",
    "r1 = 0.5\n",
    "\n",
    "def reweight(pi,q1=0.5,r1=0.5):\n",
    "    r0 = 1-r1\n",
    "    q0 = 1-q1\n",
    "    tot = pi*(q1/r1)+(1-pi)*(q0/r0)\n",
    "    w = pi*(q1/r1)\n",
    "    w /= tot\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correcting for biased sample\n",
    "\n",
    "wine_df_original[\"pred_prob_imb_corr\"] = wine_df_original[\"pred_prob_imb\"].apply(reweight,args=(q1,r1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "wine_df_original.boxplot([\"pred_prob\",\"pred_prob_imb\",\"pred_prob_imb_corr\"],by=\"quality\",layout=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_new = [1 if pi >= 0.5 else 0 for pi in wine_df_original[\"pred_prob_imb_corr\"] ]\n",
    "\n",
    "#confusion matrix\n",
    "cm =  confusion_matrix(y_pred=y_pred_new, y_true=y, labels=[0,1])\n",
    "# Plotting confusion matrix (custom help function)\n",
    "plot_confusion_matrix(cm, [\"Bad/Average Wine\", \"Great Wine\"]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "Everything that applies to regression applies to logistic regression: ridge, lasso etc penalties can be used for high-dimensional feature spaces and when they are convex the resultant loss function is too and the algorithms are efficient \n",
    "\n",
    "Indeed, the `LogisticRegression` function by default includes the penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass classification\n",
    "\n",
    "Often the output variable is categorical with many options, not just two, e.g. the wine dataset. \n",
    "\n",
    "It is more convenient to encode such multiclass output using the 1-hot encoding, i.e., each output is a vector of 0s with a single 1 in the chosen class: \n",
    "\n",
    "$$\\by_i = (y_{i1},\\ldots,y_{iK})^T \\quad y_{ij} \\in \\{0,1\\}, \\quad \\sum_j y_{ij} = 1$$\n",
    "\n",
    "\n",
    "where the labels $1,2,\\ldots,K$ are arbitrary encodings for the different output categories and any sensible analysis should not depends on their values. \n",
    "\n",
    "Binary classification takes $K=2$. In fact, in modern applications $K \\sim 100$ or even $K \\sim 1000$ (e.g. *recommendation*).\n",
    "\n",
    "The most direct extension of the binary regression to multiclass is as follows. We take\n",
    "\n",
    "$$\\by_i \\sim Categorical(\\pi_{i1},\\ldots,\\pi_{iK})$$\n",
    "\n",
    "with density \n",
    "\n",
    "$$p(\\by_i) = \\prod_{j} \\pi_{ij}^{y_{ij}}$$\n",
    "\n",
    "which is a clever way to simply say that the probability that the $j$th category is chosen is $\\pi_{ij}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial-logistic regression\n",
    "\n",
    "We need to map the probabilities $\\pi_{ij}$ to the input $\\bx_i$. One way that collapses to logistic regression when $K=2$ is to take:\n",
    "\n",
    "$$\\log{\\pi_{ij} \\over \\pi_{i1}} = f(\\bx_i,\\bb_j)$$\n",
    "\n",
    "Note that implicit in this definition is that the odds to choose $j$ vs 1 do not depend on what other options there exist: this is known as the *independence of irrelevant alternatives* assumptions and is criticized in certain contexts. \n",
    "\n",
    "The model definition implies\n",
    "\n",
    "$$\\pi_{ij} = {e^{f(\\bx_i,\\bb_j)} \\over 1+ \\sum_{k>1} e^{f(\\bx_i,\\bb_k)}}$$\n",
    "\n",
    "and you should check that for $K=2$ this is precisely logistic regression. The pivot category is taken above to be 1, but any other can be chosen - this only affects the interpretation of the results. Note also that we have different parameters $\\bb_j$ for each category $j$. \n",
    "\n",
    "This model is known by a multitude of names...\n",
    "\n",
    "https://en.wikipedia.org/wiki/Multinomial_logistic_regression\n",
    "\n",
    "The negative log-likelihood is immediatelly obtained and is **convex** in the $\\bb_j$s, hence we have a nice learning problem to solve. In fact, an old clever trick can be used to turn learning this model into a Poisson GLM, this is known as the *Poisson trick* in the Stats community. This is particularly clever for large $K$ and is being reinvented (again..) by the ML community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reanalyzing the wine data\n",
    "\n",
    "`LogisticRegression` in `sklearn` does in fact also fit the multinomial-logistic regression model. Lets try this out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multinomial-logistic regression for wine\n",
    "model = LogisticRegression(C=100,multi_class=\"multinomial\",solver=\"newton-cg\",max_iter=10000) \n",
    "wine_df = pd.read_csv(\"wine.csv\")\n",
    "X = wine_df.drop('quality', axis =1)\n",
    "y = wine_df.quality\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probabilities = cross_val_predict(model, X, y, method='predict_proba', cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cross_val_predict(model, X, y, cv = 5)\n",
    "cm =  confusion_matrix(y_pred=y_pred, y_true=y, labels=[3,4,5,6,7,8])\n",
    "# Plotting confusion matrix (custom help function)\n",
    "plot_confusion_matrix(cm, [\"3\",\"4\",\"5\",\"6\",\"7\",\"8\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the shrinkage towards the 5!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some hints for the practitioners\n",
    "\n",
    "+ Predictive modelling with categorical output can be done using `LogisticRegression`; by default this module includes regularization, hence it is straighforward to include hundreds of features\n",
    "+ When the output is multicategorical is way more sensible to build directly a model for the original output than first turn it (more or less arbitrarily) into a binary output. Even if for commercial/interpretability purposes a binary prediction is preferred, is preferrable to turn the multicategorical prediction into binary rather than the multicategorical output to binary and build a model\n",
    "+ A probabilistic classifier returns probabilities for the possible categories. It is not the data scientist's job to turn those into class predictions. This should be done in conjunction with the user of the analysis and the consideration of losses. Once the losses have been specified a simple formula gives the optimal conversion\n",
    "+ Class imbalance is an issue relevant for many or even most classification applications. Using the `class_weight=\"balanced\"` within `LogisticRegression` gives a possible improvement using oversampling - make sure to correct the probabilities it returns since they are not correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming project: forest cover classification\n",
    "\n",
    "In this project, you have to predict the class of forest cover (the predominant kind of tree cover) from strictly cartographic and environment variables.\n",
    "\n",
    "The actual forest cover type for a given observation (30 x 30 meter cell) was determined from US Forest Service (USFS) Region 2 Resource Information System (RIS) data. Independent variables were derived from data originally obtained from US Geological Survey (USGS) and USFS data. Data is in raw form (not scaled) and contains categorical data for qualitative independent variables (wilderness areas and soil types).\n",
    "\n",
    "You have further details on the data at *covertype.info* file and at https://archive.ics.uci.edu/ml/datasets/Covertype\n",
    "\n",
    "Be aware that the final dataset has been slighly modified from the original source data.\n",
    "\n",
    "As performance metric, you can use *AUC* in the binary classification case. For multi-class classification, check as well the confussion matrix and estimate the misclassification probabilities of different classes(check *metrics.confusion_matrix* and *metrics.classification_report* in *sklearn*).\n",
    "\n",
    "+ Using *MultiClass_Train.csv* file build a predictive model for *Cover_Type* .\n",
    "+ Try a first a binary classification to predict class #7 (Krummholz, https://en.wikipedia.org/wiki/Krummholz), which is a rare valuable habitat. After that, then try multi-variate classification to predict all classes.\n",
    "+ For this analysis there is an extra test dataset. Once your code is submitted we will run a competition to see how you score in the test data. Hence have prepared also the necessary script to compute the accuracy estimate on the test data once released.\n",
    "\n",
    "You can follow those **steps** in your first implementation:\n",
    "1. *Explore* and understand the dataset. \n",
    "2. Create *dummy variables* for relevant categorical features\n",
    "3. Reformat the Class_type variable into a binary one, being class #7 the target variable versus the others.\n",
    "4. *Build* an initial binary model for class #7 versus the other ones and test it on the same input data. Try to improve it using methods to tackle class imbalance.\n",
    "5. Assess expected accuracy using *cross-validation*\n",
    "6. Report which variable impacts more on results \n",
    "7. Repeat 4., 5., 6. now with a multi-class model\n",
    "8. Prepare the code to *run* on a new input file and be able to report accuracy, following same preparation steps (missing data, dummies, etc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here (click on the window and type 'b' if you want to split in more than one code window)\n",
    "\n",
    "# Step 1: Read and explore data\n",
    "\n",
    "\n",
    "# Step 2: Create dummies for relevant features\n",
    "\n",
    "\n",
    "# Step 3: Reformat the data to have an binary class target (class #7 is the target to predict)\n",
    "\n",
    "\n",
    "# Step 4: Build a binary model to predict class #7 (refine considering class imbalance methods)\n",
    "\n",
    "\n",
    "# Step 5: Assess expected accuracy: AUC\n",
    "\n",
    "\n",
    "# Step 6: Report variable impact\n",
    "\n",
    "\n",
    "# Step 7: Repeat 4,5,6 with a multi-class model. \n",
    "# As metrics, besides global accuracy, include the estimation of missclassification probabilities by class\n",
    "# Report accuracies of class #7 compared to the binary model\n",
    "\n",
    "\n",
    "\n",
    "# Step 8: Prepare code to run and check performance of you model using a new input data with same exact format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Hastie, T., Tibshirani, R., Friedman, J., 2009. *Elements of Statistical Learning*. 2nd Edition. Section 4.4; More advanced 3.8,3.9.  https://web.stanford.edu/~hastie/ElemStatLearn/\n",
    "\n",
    "Bishop, C.M. *Pattern recognition and machine learning*. Sections 4.2, 4.3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"bgsedsc_0.jpg\">\n",
    "$\\newcommand{\\bb}{\\boldsymbol{\\beta}}$\n",
    "$\\DeclareMathOperator{\\Gau}{\\mathcal{N}}$\n",
    "$\\newcommand{\\bphi}{\\boldsymbol \\phi}$\n",
    "$\\newcommand{\\bpi}{\\boldsymbol \\pi}$\n",
    "$\\newcommand{\\bx}{\\boldsymbol{x}}$\n",
    "$\\newcommand{\\by}{\\boldsymbol{y}}$\n",
    "$\\newcommand{\\bmu}{\\boldsymbol{\\mu}}$\n",
    "$\\newcommand{\\bS}{\\boldsymbol{\\Sigma}}$\n",
    "$\\newcommand{\\whbb}{\\widehat{\\bb}}$\n",
    "$\\newcommand{\\hf}{\\hat{f}}$\n",
    "$\\newcommand{\\hy}{\\hat{y}}$\n",
    "$\\newcommand{\\tf}{\\tilde{f}}$\n",
    "$\\newcommand{\\ybar}{\\overline{y}}$\n",
    "$\\newcommand{\\E}{\\mathbb{E}}$\n",
    "$\\newcommand{\\Var}{Var}$\n",
    "$\\newcommand{\\Cov}{Cov}$\n",
    "$\\newcommand{\\Cor}{Cor}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "Here collect important but more advanced material on classification \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "# This is a Python module that contains plotting commands\n",
    "import matplotlib.pyplot as plt\n",
    "# the following provides further tools for plotting with dfs\n",
    "import seaborn as sns \n",
    "\n",
    "import helper_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = pd.read_csv(\"spam_small_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is it needed to turn probabilities into class prediction? here is why it might not\n",
    "\n",
    "There will be applications for which there is no good reason to turn the estimated $\\pi_i$s into class predictions, $\\hy_i \\in \\{0,1\\}$\n",
    "\n",
    "Consider the following example: I am interested to forecast electoral results. On the basis of a survey on a representative sample of voters (lack of representation can also be corrected using statistical methods) I have estimated the probability that a voter with given socio-demo-geo characteristics would vote for the ruling party.  Each combination of socio-demo-geo characteristics defines a voter-stratum, e.g.\n",
    "\n",
    "man + age group 25-40 + unemployed + tertiary education + lives in x subregion of country\n",
    "\n",
    "My model predicts $\\pi_n$, the probability of voting for the ruling party for an individual in the stratum $n$. Using census data I can find out how many people in the country belong in each stratum and run a simulation where I simulate possible electoral results according to individual behaviour. In this way I obtain a *predictive distribution* on the number of votes the ruling party will take. I can equally easily obtain a predictive distribution on the number of seats they will win on the parliament. \n",
    "\n",
    "This is actually a component of how one can build a sophisticated prediction machine for elections, see e.g. \n",
    "\n",
    "https://www.barcelonagse.eu/research/working-papers/bayesian-forecasting-electoral-outcomes-new-parties-competition\n",
    "\n",
    "In this application is really not a good idea to turn class probabilities into class prediction. There are several others like this.\n",
    "\n",
    "Here is for example how in a **Data Science Center** project on Health Analytics, we communicated class uncertainty to the client: \n",
    "\n",
    "<img src=\"labels-after.png\">\n",
    "\n",
    "In subsequent analyses that required regression with this data, we used *weighted likelihood/weighted least squares*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes classifiers\n",
    "\n",
    "An entirely different - but turns out to be related (hence included here) - approach to classification is to built a **joint model** for\n",
    "\n",
    "$$\n",
    "p(\\bx,y)\n",
    "$$\n",
    "\n",
    "as opposed for the conditional \n",
    "\n",
    "$$\n",
    "p(y | \\bx)\n",
    "$$\n",
    "\n",
    "that the previous approach we consider does. The fact that the joint model gives a recipe for generating data makes this approach be referred to as **generative**. Analogous is the **inverse regression** approach to linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes classifiers come up with a joint model by decomposing the joint probabilities the other way round: \n",
    "\n",
    "$$p(\\bx,y) = p(y) p(\\bx | y)$$\n",
    "\n",
    "Focusing on binary classification, one learns \n",
    "\n",
    "1. $p(y=1)$ - this is trivial\n",
    "2. $p(\\bx | y=1)$ and $p(\\bx | y=0)$; the two conditional distributions\n",
    "\n",
    "With these, predictive probabilities for the class are obtained using the **Bayes theorem** (hence the name) \n",
    "\n",
    "$$p(y =1 | \\bx) = { p(\\bx | y=1) p(y =1) \\over p(\\bx | y=1) p(y =1)  + p(\\bx | y=0) p(y =0)} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge is to come up with tractable and useful models for $p(\\bx |y)$ - non-trivial since we typically have 10ths/100ds/1000ds of features\n",
    "\n",
    "Two off-the-shelf options are: \n",
    "\n",
    "1. $\\bx | y = i \\sim \\Gau(\\bmu_i, \\bS)$, for $i=0,1$; why common variance? The resultant classifier is known as **Fisher discriminant analysis**\n",
    "2. $p(\\bx | y = i)  = \\prod_{j=1}^p p_{i,j}(x_j)$ for $\\bx = (x_1,\\ldots,x_p)^T$; the resultant classifier is known as **naive Bayes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is well known, e.g. since Efron (1975, JASA), that discriminant analysis is equivalent to logistic regression with specific coefficients - the article shows that it is not that good idea to use the former\n",
    "\n",
    "Naive Bayes is not functionally related to logistic regression but theory exists about their relative performance. In a nutshell, naive Bayes classifiers reach near-optimal performance with smaller sample sizes but their optimal performance is worse than that of logistic regression\n",
    "\n",
    "Still, subject matter knowledge and more clever modelling on $p(\\bx|y)$ can improve the performance of Bayes classifiers\n",
    "\n",
    "Lets revisit an analysis we did with the spam dataset and appreciate the implicity Bayes classifier feel to it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam.boxplot(column=[\"word_freq_you\",\"word_freq_hp\",\"char_freq_!\"], by = \"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal regression\n",
    "\n",
    "This is to deal with ordinal data, i.e., data $y_i$ that take $K$ distinct values, say from 1 to $K$, but where values are comparable, i.e., $k$ is less than $k+1$, e.g., wine quality data. This is in-between regression and classification. We can adapt the multiclass and regression ideas to this context. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative models\n",
    "\n",
    "This is the most direct such adaptation of linear regression and multiclass to ordinal data. The comulative logistic model specifies that: \n",
    "\n",
    "$$p(y_i = j) = {1 \\over 1 + e^{f(\\bx_i,\\bb) - a_j}}  - {1 \\over 1 + e^{f(\\bx_i,\\bb) - a_{j-1}}}$$\n",
    "\n",
    "where as usual $f(\\bx_i,\\bb) = \\bphi_i^T \\bb$ but where in this case $\\bphi$ *does not* contain an intercept - this is the role of the $a_j$s. \n",
    "\n",
    "The model is equivalent to:\n",
    "\n",
    "$$P(y_i \\leq j) = {1 \\over 1 + e^{f(\\bx_i,\\bb) - a_j}}$$\n",
    "\n",
    "and you can notice the connection to logistic regression. A couple of observations: \n",
    " + Note the common $\\bb$ across levels: this is in line with linear regression\n",
    " + By expanding the linear predictor we write above that  \n",
    " $$P(y_i \\leq j) = {1 \\over 1 + e^{\\bphi_i^T\\bb - a_j}}$$\n",
    " whereas the logistic binary regression would write as:\n",
    " $$P(y_i = 1) = {1 \\over 1 + e^{-\\bphi_i^T\\bb - a_j}}$$\n",
    " The reason for the different sign is for interpretation only (as a predictive model this makes no difference): if a feature $j$ is positively correlated with $y_i$ then in the proposed formulation the larger $\\beta_j$ the more to the right the distribution of $y_i$ shifts.\n",
    " + This model also can be estimated using efficient optimization\n",
    " + This model can be equivalently formulated in terms of **latent variables** and threshold levels $a_1,\\ldots,a_K$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last glass of wine... \n",
    "\n",
    "Incredibly (but consistently with its poor support for good quality Stats) there is no module in Python (not just sklearn) for ordinal regression. I have my own functions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
